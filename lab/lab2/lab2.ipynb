{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28dfab30-93f8-4eef-a8be-327f4b4dfc72",
   "metadata": {},
   "source": [
    "# BME-1301 Lab-2: Segmentation using U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca2c07d8-7fcb-4a76-a700-1aedd0318a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## WRITE YOUR ANSWER BELOW ########################\n",
    "STUDENT_NAME = 'Example'\n",
    "STUDENT_NUMBER = '1234567890'\n",
    "#########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4bf8c38-2bfd-499a-8a88-2bb5a45fadf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5c682a55-81df-47f5-81b0-beb80d3e0048",
   "metadata": {},
   "source": [
    "## Setup environments\n",
    "You are required to install the following packages.\n",
    "- jupyter\n",
    "- scikit-image\n",
    "- numPy\n",
    "- pytorch torchvision\n",
    "- matplotlib\n",
    "- tqdm\n",
    "- scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc03c4be-0d16-4a54-af78-fbb43f689b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary packages for this homework, you are free to import more.\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import sklearn\n",
    "\n",
    "# packages of this lab\n",
    "from bme1301 import lab2 as lab\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "print('Importing successful')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8657c185-3ebf-477e-ae6b-41931ee42ec0",
   "metadata": {},
   "source": [
    "## Part 1: Dataset preparation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9052feea-4844-4c29-8e76-605352a50f02",
   "metadata": {},
   "source": [
    "**Dataset for training**\n",
    "1. Initialize the variables of the dataset.\n",
    "2. Obtain the pair of the original image and GT. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc069434-33cf-455e-b97c-b16cabc5d0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms as T\n",
    "from torchvision.transforms import functional as F\n",
    "\n",
    "class ImageFolder(data.Dataset):\n",
    "    def __init__(self, root, mode='train', augmentation_prob=0.4):\n",
    "        assert mode in {'train'}\n",
    "        \"\"\"Initializes image paths and preprocessing module.\"\"\"\n",
    "        self.root = root\n",
    "\n",
    "        # GT : Ground Truth\n",
    "        self.GT_dir = os.path.join(root,'GT/')\n",
    "        self.image_names = os.listdir(self.GT_dir)\n",
    "        self.len = len(self.image_names)\n",
    "        self.GT_paths = [os.path.join(self.GT_dir,self.image_names[i]) for i in range(self.len)]\n",
    "        \n",
    "        #Generate the paths of input images.\n",
    "        ######################## WRITE YOUR ANSWER BELOW ########################\n",
    "        self.image_dir = None\n",
    "        self.image_paths = None\n",
    "        #########################################################################\n",
    "        self.mode = mode\n",
    "        self.RotationDegree = [0, 90, 180, 270]\n",
    "        self.augmentation_prob = augmentation_prob\n",
    "        print(\"image count in {} path :{}\".format(self.mode, self.len))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Reads an image from a file and preprocesses it and returns.\"\"\"\n",
    "        image_path = self.image_paths[index]\n",
    "        filename = self.image_names[index]\n",
    "        GT_path = self.GT_paths[index]\n",
    "\n",
    "        image = Image.open(image_path)\n",
    "        seg_gt = Image.open(GT_path)\n",
    "\n",
    "        aspect_ratio = image.size[1] / image.size[0]\n",
    "\n",
    "        Transform = []\n",
    "\n",
    "        ResizeRange = random.randint(300, 320)\n",
    "        Transform.append(T.Resize((int(ResizeRange * aspect_ratio), ResizeRange)))\n",
    "        p_transform = random.random()\n",
    "\n",
    "        if (self.mode == 'train') and p_transform <= self.augmentation_prob:\n",
    "            \n",
    "            RotationDegree = random.randint(0, 3)\n",
    "            RotationDegree = self.RotationDegree[RotationDegree]\n",
    "            if (RotationDegree == 90) or (RotationDegree == 270):\n",
    "                aspect_ratio = 1 / aspect_ratio\n",
    "            \n",
    "            Transform.append(T.RandomRotation((RotationDegree, RotationDegree)))\n",
    "            \n",
    "            #use randint and T.RandomRotation to rotate the image by (-10,10) degrees.\n",
    "            ######################## WRITE YOUR ANSWER BELOW ########################\n",
    "            RotationRange = None\n",
    "            Transform.append(None)\n",
    "            #########################################################################\n",
    "            \n",
    "            CropRange = random.randint(250, 270)\n",
    "            Transform.append(T.CenterCrop((int(CropRange * aspect_ratio), CropRange)))\n",
    "            Transform = T.Compose(Transform)\n",
    "            \n",
    "            image = Transform(image)\n",
    "            \n",
    "            #Be careful: when you do geometric transformation on the original image,you need to do\n",
    "            #the same transform on the gt, to keep the consistency.\n",
    "            ######################## WRITE YOUR ANSWER BELOW ########################\n",
    "            seg_gt = None\n",
    "            #########################################################################\n",
    "            \n",
    "\n",
    "            ShiftRange_left = random.randint(0, 20)\n",
    "            ShiftRange_upper = random.randint(0, 20)\n",
    "            ShiftRange_right = image.size[0] - random.randint(0, 20)\n",
    "            ShiftRange_lower = image.size[1] - random.randint(0, 20)\n",
    "            image = image.crop(box=(ShiftRange_left, ShiftRange_upper, ShiftRange_right, ShiftRange_lower))\n",
    "            seg_gt = seg_gt.crop(box=(ShiftRange_left, ShiftRange_upper, ShiftRange_right, ShiftRange_lower))\n",
    "\n",
    "            if random.random() < 0.5:\n",
    "                image = F.hflip(image)\n",
    "                seg_gt = F.hflip(seg_gt)\n",
    "\n",
    "            if random.random() < 0.5:\n",
    "                image = F.vflip(image)\n",
    "                seg_gt = F.vflip(seg_gt)\n",
    "            \n",
    "            \n",
    "            #use T.ColorJitter to do color transform here. You can't change the color\n",
    "            #of gt! Set brightness=0.2, contrast=0.2, hue=0.02.\n",
    "            ######################## WRITE YOUR ANSWER BELOW ########################\n",
    "            Transform = None\n",
    "            #########################################################################\n",
    "            image = Transform(image)\n",
    "\n",
    "            Transform = []\n",
    "\n",
    "        Transform.append(T.Resize((int(256 * aspect_ratio) - int(256 * aspect_ratio) % 16, 256)))\n",
    "        Transform.append(T.ToTensor())\n",
    "        Transform = T.Compose(Transform)\n",
    "\n",
    "        image = Transform(image)\n",
    "        seg_gt = Transform(seg_gt)\n",
    "\n",
    "        Norm_ = T.Normalize((0.5), (0.5))\n",
    "        image = Norm_(image)\n",
    "\n",
    "        return image, seg_gt\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of font files.\"\"\"\n",
    "        return self.len\n",
    "    \n",
    "\n",
    "    \n",
    "train_folder = ImageFolder(root = 'dataset/ACDC-2D-onelabel/train/' , mode = 'train')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ef7c776-3288-473e-aaae-4902b8edf82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test_ImageFolder(data.Dataset):\n",
    "    def __init__(self, root, mode='val'):\n",
    "        \"\"\"Initializes image paths and preprocessing module.\"\"\"\n",
    "        assert mode in {'val','test'}\n",
    "        self.root = root\n",
    "\n",
    "        \n",
    "        self.GT_dir = os.path.join(root,'GT/')\n",
    "        self.image_names = os.listdir(self.GT_dir)\n",
    "        self.len = len(self.image_names)\n",
    "        self.GT_paths = [os.path.join(self.GT_dir,self.image_names[i]) for i in range(self.len)]\n",
    "        \n",
    "        \n",
    "        self.image_dir = os.path.join(root,'Img/')\n",
    "        self.image_paths = [os.path.join(self.image_dir ,self.image_names[i]) for i in range(self.len)]\n",
    "        \n",
    "        self.mode = mode\n",
    "        \n",
    "        print(\"image count in {} path :{}\".format(self.mode, len(self.image_paths)))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Reads an image from a file and preprocesses it and returns.\"\"\"\n",
    "        image_path = self.image_paths[index]\n",
    "        filename = self.image_names[index]\n",
    "        GT_path = self.GT_paths[index]\n",
    "\n",
    "        image = Image.open(image_path)\n",
    "        seg_gt = Image.open(GT_path)\n",
    "\n",
    "        image = Image.open(image_path)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        Transform = []\n",
    "        \n",
    "\n",
    "        \n",
    "        Transform.append(T.ToTensor())\n",
    "        Transform = T.Compose(Transform)\n",
    "\n",
    "        image = Transform(image)\n",
    "        \n",
    "        seg_gt = Transform(seg_gt)\n",
    "        Norm_ = T.Normalize((0.5), (0.5))\n",
    "        image = Norm_(image)\n",
    "        return image,seg_gt\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of font files.\"\"\"\n",
    "        return self.len\n",
    "\n",
    "#Get val_folder and test_folder using Test_ImageFolder.\n",
    "######################## WRITE YOUR ANSWER BELOW ########################\n",
    "val_folder = None\n",
    "test_folder = None\n",
    "#########################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5cc78c-cc27-43e2-b73e-2bbb73a4044b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_loader(image_root_path, batch_size, num_workers=0, mode='train', augmentation_prob=0.4):\n",
    "    \"\"\"Builds and returns Dataloader.\"\"\"\n",
    "    if mode=='test' or mode == 'val':\n",
    "        dataset = Test_ImageFolder(root=image_root_path, mode=mode, augmentation_prob=augmentation_prob)\n",
    "    else:\n",
    "        dataset = ImageFolder(root=image_root_path, mode=mode, augmentation_prob=augmentation_prob)\n",
    "\n",
    "    data_loader = data.DataLoader(dataset=dataset,\n",
    "                                batch_size=batch_size,\n",
    "                                shuffle=True if mode == 'train' else False,\n",
    "                                num_workers=num_workers)\n",
    "\n",
    "    return data_loader\n",
    "train_loader = get_loader(image_root_path='dataset/ACDC-2D-onelabel/train/', batch_size=10, mode='train')\n",
    "#Get val_loader and test_loader using get_loader.\n",
    "######################## WRITE YOUR ANSWER BELOW ########################\n",
    "val_loader = None\n",
    "test_loader = None\n",
    "#########################################################################\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ae76e5-0dc3-47a8-9730-397249013244",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def fetch_batch_sample(loader, idx):\n",
    "    batch = next(itertools.islice(loader, idx, None))\n",
    "    return batch\n",
    "\n",
    "batch = fetch_batch_sample(train_loader, idx=0)\n",
    "image, seg_gt = batch\n",
    "print(image.shape)\n",
    "print(seg_gt.shape)\n",
    "lab.imsshow(image[:,0,  :, :])\n",
    "lab.imsshow(seg_gt[:,0, :, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dcbdd9-3976-40de-a08c-12380c41c4e0",
   "metadata": {},
   "source": [
    "**Range of values**\n",
    "1. Images' intensities are normalized to $[-1, 1]$\n",
    "2. Segmentation ground truths are defined between $[0, 1]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ae97c6-1135-418a-b56e-ba70ad1d7dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lab.imsshow([image[5, 0, :, :], seg_gt[5, 0, :, :]], is_colorbar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f90f5e6-2c20-49a6-b8ab-fbcae0879ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = lab.fetch_batch_sample(val_loader, idx=0)\n",
    "image, seg_gt = batch\n",
    "print(image.shape)\n",
    "print(seg_gt.shape)\n",
    "lab.imsshow(image[:, 0, :, :])\n",
    "lab.imsshow(seg_gt[:, 0, :, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca38344-6cd5-443b-8999-4142d61418f2",
   "metadata": {},
   "source": [
    "## Part 2: Evaluation Metrics for Segmentation Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7365ec3a-69fd-4a12-b34e-f9cccf3e4851",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask1 = Image.open('assets/mask1.png')\n",
    "mask1 = np.array (mask1)>0\n",
    "lab.imgshow(mask1)\n",
    "\n",
    "mask2 = Image.open('assets/mask2.png')\n",
    "mask2 = np.array(mask2)>0\n",
    "lab.imgshow(mask2)\n",
    "\n",
    "mask1 = torch.tensor(mask1).unsqueeze(0).unsqueeze(0)\n",
    "mask2 = torch.tensor(mask2).unsqueeze(0).unsqueeze(0)\n",
    "print(mask1.shape)\n",
    "print(mask2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2eb5e5-8a8a-432d-ab8e-7b2947f09a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bme1312.evaluation as evaluation\n",
    "\n",
    "def accuracy(SR, GT, threshold=0.5):\n",
    "    SR = SR > threshold\n",
    "    GT = GT == torch.max(GT)\n",
    "    corr = torch.sum(SR == GT)\n",
    "    tensor_size = SR.size(0) * SR.size(1) * SR.size(2) * SR.size(3)\n",
    "    acc = float(corr) / float(tensor_size)\n",
    "\n",
    "    return acc\n",
    "\n",
    "your_acc = accuracy(mask1,mask2)\n",
    "std_acc = evaluation.get_accuracy(mask1,mask2)\n",
    "print(\"your_acc:\",your_acc)\n",
    "print(\"std_acc:\",std_acc)\n",
    "assert(abs(your_acc-std_acc)<1e-6) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d19523-3e2f-434c-b252-58cb97a94356",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def sensitivity(SR, GT, threshold=0.5):\n",
    "    # Sensitivity == Recall\n",
    "    SR = SR > threshold\n",
    "    GT = GT == torch.max(GT)\n",
    "\n",
    "    # TP : True Positive\n",
    "    # FN : False Negative\n",
    "    TP = ((SR == 1) & (GT == 1))\n",
    "    #calculate FN\n",
    "    ######################## WRITE YOUR ANSWER BELOW ########################\n",
    "    FN = None\n",
    "    #########################################################################\n",
    "    \n",
    "\n",
    "    # print(\"torch.sum(TP)\",torch.sum(TP))\n",
    "    # print(\"torch.sum(TP + FN)\",torch.sum(TP + FN))\n",
    "\n",
    "    SE = float(torch.sum(TP)) / (float(torch.sum(TP + FN)) + 1e-6)\n",
    "\n",
    "    return SE\n",
    "\n",
    "your_sen = sensitivity(mask1,mask2)\n",
    "std_sen = evaluation.get_sensitivity(mask1,mask2)\n",
    "print(\"your_sen:\",your_sen)\n",
    "print(\"std_sen:\",std_sen)\n",
    "assert(abs(your_sen-std_sen)<1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53cfb97-70be-4842-aa05-0feb2b5ba460",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def specificity(SR, GT, threshold=0.5):\n",
    "    SR = SR > threshold\n",
    "    GT = GT == torch.max(GT)\n",
    "\n",
    "    # TN : True Negative\n",
    "    # FP : False Positive\n",
    "    \n",
    "    #calculate TN and FP\n",
    "    ######################## WRITE YOUR ANSWER BELOW ########################\n",
    "    TN = None\n",
    "    FP = None\n",
    "    #########################################################################\n",
    "    \n",
    "\n",
    "    SP = float(torch.sum(TN)) / (float(torch.sum(TN + FP)) + 1e-6)\n",
    "\n",
    "    # print(\"------------------specificity---------------\")\n",
    "    # print(\"TN:\",torch.sum(TN))\n",
    "    # print(\"FP:\",torch.sum(FP))\n",
    "    # print(\"SP:\",SP)\n",
    "    return SP\n",
    "\n",
    "\n",
    "your_spe = specificity(mask1,mask2)\n",
    "std_spe = evaluation.get_specificity(mask1,mask2)\n",
    "print(\"your_spe:\",your_spe)\n",
    "print(\"std_spe:\",std_spe)\n",
    "assert(abs(your_spe-std_spe)<1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17361f5f-5623-45b1-b4f9-40c6c4654338",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(SR, GT, threshold=0.5):\n",
    "    SR = SR > threshold\n",
    "    GT = GT == torch.max(GT)\n",
    "\n",
    "    # TP : True Positive\n",
    "    # FP : False Positive\n",
    "    TP = ((SR == 1) & (GT == 1))\n",
    "    FP = ((SR == 1) & (GT == 0))\n",
    "    \n",
    "    #calculate precision. use eps=1e-6 to avoid zero division.\n",
    "    ######################## WRITE YOUR ANSWER BELOW ########################\n",
    "    PC = None\n",
    "    #########################################################################\n",
    "    \n",
    "\n",
    "    # print(\"------------------precision---------------\")\n",
    "    # print(\"TP:\", torch.sum(TP))\n",
    "    # print(\"FP:\", torch.sum(FP))\n",
    "    # print(\"PC:\", PC)\n",
    "    return PC\n",
    "\n",
    "your_prec = precision(mask1,mask2)\n",
    "std_prec = evaluation.get_precision(mask1,mask2)\n",
    "print(\"your_prec:\",your_prec)\n",
    "print(\"std_prec:\",std_prec)\n",
    "assert(abs(your_prec-std_prec)<1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a1c317-1213-49ea-a5d9-7a9712bc2cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def F1(SR, GT, threshold=0.5):\n",
    "    \n",
    "    SE = sensitivity(SR, GT, threshold=threshold)\n",
    "    PC = precision(SR, GT, threshold=threshold)\n",
    "    \n",
    "    #Calculate F1. use eps=1e-6 to avoid zero division.\n",
    "    ######################## WRITE YOUR ANSWER BELOW ########################\n",
    "    F1 = None\n",
    "    #########################################################################\n",
    "    \n",
    "\n",
    "    return F1\n",
    "\n",
    "your_F1 = F1(mask1,mask2)\n",
    "std_F1 = evaluation.get_F1(mask1,mask2)\n",
    "print(\"your_F1:\",your_F1)\n",
    "print(\"std_F1:\",std_F1)\n",
    "assert(abs(your_F1-std_F1)<1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fb6a2a-fbe1-4ad3-98a0-7b56c38b948e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def JS(SR, GT, threshold=0.5):\n",
    "    # JS : Jaccard similarity\n",
    "    SR = SR > threshold\n",
    "    GT = GT == torch.max(GT)\n",
    "\n",
    "    Inter = torch.sum(((SR == 1) & (GT == 1)))\n",
    "    \n",
    "    #Calculate Union between SR and GT.\n",
    "    ######################## WRITE YOUR ANSWER BELOW ########################\n",
    "    Union = None\n",
    "    #########################################################################\n",
    "    JS = float(Inter) / (float(Union) + 1e-6)\n",
    "\n",
    "    return JS\n",
    "\n",
    "your_JS = JS(mask1,mask2)\n",
    "std_JS = evaluation.get_JS(mask1,mask2)\n",
    "print(\"your_JS:\",your_JS)\n",
    "print(\"std_JS:\",std_JS)\n",
    "assert(abs(your_JS-std_JS)<1e-6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c5be83-851c-467c-bcdb-b5e40434ba40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DC(SR, GT, threshold=0.5):\n",
    "    # DC : Dice Coefficient\n",
    "    SR = SR > threshold\n",
    "    GT = GT == torch.max(GT)\n",
    "\n",
    "\n",
    "    Inter = torch.sum(((SR==1) & (GT==1)))\n",
    "    #calculate DC.\n",
    "    ######################## WRITE YOUR ANSWER BELOW ########################\n",
    "    DC = None\n",
    "    #########################################################################\n",
    "    \n",
    "\n",
    "    return DC\n",
    "\n",
    "your_DC = DC(mask1,mask2)\n",
    "std_DC = evaluation.get_DC(mask1,mask2)\n",
    "print(\"your_DC:\",your_DC)\n",
    "print(\"std_DC:\",std_DC)\n",
    "assert(abs(your_DC-std_DC)<1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e717b16-e975-4f7d-9879-0283abdd8541",
   "metadata": {},
   "source": [
    "## Part 3: U-Net\n",
    "\n",
    "Original            |  Simplification\n",
    ":-------------------------:|:-------------------------:\n",
    "![](./assets/unet-validconv.png)  |  ![](./assets/unet-simplification.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b3fb85-ee6f-4140-a014-434ca7cb3805",
   "metadata": {},
   "source": [
    "### 3.1 MultiConv\n",
    "<center>\n",
    "    <img src=\"./assets/DoubleConv.png\" width=\"80%\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575ca707-336d-411b-bbcf-a728a505211b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        #use nn.Sequential(),BatchNorm2d,ReLU. Use padding to keep the size of image.\n",
    "        ######################## WRITE YOUR ANSWER BELOW ########################\n",
    "        pass\n",
    "        #########################################################################\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [B, C_in, H, W]\n",
    "        out: [B, C_out, H, W]\n",
    "        \"\"\"\n",
    "        out = self.layers(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "net = DoubleConv(in_channels=2, out_channels=64)\n",
    "x = torch.randn(5, 2, 160, 160)\n",
    "out = net(x)\n",
    "assert (5, 64, 160, 160) == out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3f70f9-3d72-4ed8-984a-305adbffea21",
   "metadata": {},
   "source": [
    "### 3.2 Down/Up Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e0c3ef-e5aa-497e-9805-b1d8ce38da29",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"./assets/Down.png\" width=\"80%\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b6b2f1-87c6-47aa-8eb1-37066b8bbda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        #use nn.Sequential,nn.MaxPool2d and DoubleConv defined by you.\n",
    "        ######################## WRITE YOUR ANSWER BELOW ########################\n",
    "        self.maxpool_conv = None\n",
    "        #########################################################################\n",
    "\n",
    "    def forward(self, x):        \n",
    "        ######################## WRITE YOUR ANSWER BELOW ########################\n",
    "        return None\n",
    "        #########################################################################\n",
    "\n",
    "\n",
    "net = Down(in_channels=64, out_channels=128)\n",
    "x = torch.randn(5, 64, 160, 160)\n",
    "out = net(x)\n",
    "assert (5, 128, 80, 80) == out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af28465-2192-4c20-903b-71a19835c049",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"./assets/Up.png\" width=\"80%\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2330864-de1e-4b3a-8954-c14f382e6488",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()        \n",
    "        \n",
    "        #use nn.ConvTranspose2d for upsampling.\n",
    "        ######################## WRITE YOUR ANSWER BELOW ########################\n",
    "        self.up = None\n",
    "        #########################################################################\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "        \n",
    "\n",
    "    def forward(self, x1, x2):        \n",
    "        \n",
    "        \n",
    "        x1 = self.up(x1)\n",
    "        H1, W1 = x1.shape[2:]\n",
    "        H2, W2 = x2.shape[2:]\n",
    "        \n",
    "        # print('before padding:', x1.shape)\n",
    "        \n",
    "        #use F.pad to change the shape of x1.\n",
    "        ######################## WRITE YOUR ANSWER BELOW ########################\n",
    "        x1 = None\n",
    "        #########################################################################\n",
    "        # print('after padding: ',x1.shape)\n",
    "\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        out = self.conv(x)\n",
    "        \n",
    "        return out\n",
    "\n",
    "net = Up(in_channels=128, out_channels=64)\n",
    "x = torch.randn(5, 128, 80, 80)\n",
    "x_res = torch.randn(5, 64, 160, 160)\n",
    "out = net(x, x_res)\n",
    "assert (5, 64, 160, 160) == out.shape\n",
    "\n",
    "\n",
    "net = Up(in_channels=128, out_channels=64)\n",
    "x = torch.randn(5, 128, 196, 196)\n",
    "x_res = torch.randn(5, 64, 568, 568)\n",
    "out = net(x, x_res)\n",
    "assert (5, 64, 568, 568) == out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08160cd-84b1-4871-8e4c-8d13646b934f",
   "metadata": {},
   "source": [
    "### 3.3 Compose together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b85cd1-56df-4525-adde-123591c02e0e",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"./assets/unet-simplification.png\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbb31c3-4c4b-492e-a8de-9b99caebc3b9",
   "metadata": {},
   "source": [
    "Trick for manipulating dimension of channel: 1x1 convolution as projection\n",
    "<center>\n",
    "    <img src=\"./assets/1x1conv_projection.png\" width=\"60%\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5aacbb-0e28-41b3-9083-d430301049f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, C_base=64):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "        #Use class defined by you. Be careful about params here.\n",
    "        ######################## WRITE YOUR ANSWER BELOW ########################\n",
    "        self.in_conv = DoubleConv(n_channels, C_base)\n",
    "\n",
    "        self.down1 = None\n",
    "        self.down2 = None\n",
    "        self.down3 = None\n",
    "        self.down4 = None\n",
    "        self.up1 = None\n",
    "        self.up2 = None\n",
    "        self.up3 = None\n",
    "        self.up4 = None\n",
    "        #########################################################################\n",
    "        self.out_projection = nn.Conv2d(C_base, n_classes, kernel_size=1)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: [B, n_channels, H, W]\n",
    "        :return [B, n_classes, H, W]\n",
    "        \"\"\" \n",
    "        ######################## WRITE YOUR ANSWER BELOW ########################\n",
    "        pass\n",
    "        #########################################################################\n",
    "        pred = self.out_projection(x)        \n",
    "        \n",
    "        return pred\n",
    "\n",
    "\n",
    "net = UNet(n_channels=1, n_classes=1)\n",
    "x = torch.randn(5, 1, 256, 256)\n",
    "out = net(x)\n",
    "assert (5, 1, 256, 256) == out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734e4c4e-d142-42a0-bc6c-012f8a6dbc3b",
   "metadata": {},
   "source": [
    "## Part 4: Binary cross-entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0e38c8-5630-43d3-b98d-3ba3e215f561",
   "metadata": {},
   "source": [
    "Step 1: normalize the predictions (output from network)\n",
    "<center>\n",
    "    <img src=\"./assets/sigmoid.png\", width=\"80%\">\n",
    "</center>\n",
    "\n",
    "Step 2: use binary cross entropy to measure the distance\n",
    "1. Each pixel's loss value can be defined as:\n",
    "$$\n",
    " l_n = - \\left[ y_n \\cdot \\log x_n + (1 - y_n) \\cdot \\log (1 - x_n) \\right]\n",
    "$$\n",
    "2. Using `mean` reduction to compute averaged loss\n",
    "$$\n",
    "\\ell(x, y) = L = \\frac{1}{N} \\sum(\\{l_1,\\dots,l_N\\})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eeb02dce-351f-459f-9291-66afa26c0ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyBinaryCrossEntropy(object):\n",
    "    def __init__(self):\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.bce = nn.BCELoss(reduction='mean')\n",
    "    \n",
    "    def __call__(self, pred_seg, seg_gt):\n",
    "        \"\"\"\n",
    "        :param pred_seg: [B, C=1, H, W]\n",
    "        :param seg_gt: [B, C=1, H, W]\n",
    "        \"\"\"        \n",
    "        #get the probability by sigmoid. Use BCEloss.\n",
    "        ######################## WRITE YOUR ANSWER BELOW ########################\n",
    "        pred_seg_probs = None\n",
    "        loss = None\n",
    "        #########################################################################\n",
    "        return loss\n",
    "\n",
    "\n",
    "seg_pred = torch.randn(2, 1, 256, 256)\n",
    "seg_gt = torch.randn(2, 1, 256, 256)\n",
    "criteron = MyBinaryCrossEntropy()\n",
    "loss = criteron(seg_pred, seg_gt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12f2efd-45c2-474c-97bb-a4cd31596b8c",
   "metadata": {},
   "source": [
    "## Part 5: Train you segmentation network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc48a1b3-7680-43c2-9a6d-163a09b51e29",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Construct your solver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198b06e9-d872-499f-832e-b48fb1253ff7",
   "metadata": {},
   "source": [
    "Training trick: let learning rate slowly decay usually results in better performance.\n",
    "<center>\n",
    "    <img src=\"./assets/scheduler_motivation.png\", width=\"90%\">\n",
    "</center>\n",
    "<center>\n",
    "    <img src=\"./assets/lr_scheduler.png\", width=\"60%\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b13b37-6b58-4bd8-80b9-0cf617869851",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = UNet(n_channels=1, n_classes=1, C_base=32)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=0.95)\n",
    "\n",
    "solver = lab.Solver(\n",
    "    model = net,\n",
    "    optimizer = optimizer,\n",
    "    criterion = MyBinaryCrossEntropy(),\n",
    "    lr_scheduler=lr_scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136f0a87-d5c3-4c81-8a35-11081c9cb96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = get_loader(image_root_path='dataset/ACDC-2D-onelabel/train/', batch_size=32, mode='train')\n",
    "val_loader = get_loader(image_root_path='dataset/ACDC-2D-onelabel/val/', batch_size=32, mode='val')\n",
    "test_loader = get_loader(image_root_path='dataset/ACDC-2D-onelabel/test/', batch_size=32, mode='test')\n",
    "\n",
    "solver.train(\n",
    "    epochs=50, \n",
    "    data_loader=train_loader,\n",
    "    val_loader=val_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab55a96-6ee2-47f9-928f-47c1822d12f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "solver.validate(\n",
    "    data_loader=val_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19421172-ef93-446e-b339-fe2e74c6482f",
   "metadata": {},
   "outputs": [],
   "source": [
    "solver.visualize(\n",
    "    data_loader=val_loader,\n",
    "    idx=5 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1468ec-2660-4b60-b90b-4f52a1be53f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "solver.validate(\n",
    "    data_loader=test_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0b2026-3ea8-4a6d-b163-817978f5bbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "solver.visualize(\n",
    "    data_loader=test_loader,\n",
    "    idx=10  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc5b473-66a0-4dcf-af3b-16a2b2959b9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
